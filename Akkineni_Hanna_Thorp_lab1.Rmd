---
title: "Corruption and Parking Violations"
author: "Akkineni, Hanna, Thorp"
date: "September 26, 2016"
output:
  pdf_document: default
---



```{r}
setwd("C:/Users/kevin/OneDrive/School/MIDS/W203 - Statistics for Data Science/Lab 1/W203_lab1_corruption")
#library(car)
#library(grid)
#library(ggplot2)
library(knitr)
library(kableExtra)

load("Corrupt.Rdata")

## Correct Data Problems
#Fix majoritymulsim where value = -1, should be 0
FMcorrupt[FMcorrupt$majoritymuslim == -1 & ! is.na(FMcorrupt$majoritymuslim), "majoritymuslim"] = 0

# Add missing counties.  Reference: https://www.worldatlas.com/aatlas/ctycodes.htm
FMcorrupt[FMcorrupt$wbcode == "ARE", "country"] = "UNITED ARAB EMIRATES"
FMcorrupt[FMcorrupt$wbcode == "CAF", "country"] = "CENTRAL AFRICAN REPUBLIC"
FMcorrupt[FMcorrupt$wbcode == "CAN", "country"] = "CANADA"
FMcorrupt[FMcorrupt$wbcode == "COL", "country"] = "COLUMBIA"
FMcorrupt[FMcorrupt$wbcode == "ECU", "country"] = "ECUADOR"
FMcorrupt[FMcorrupt$wbcode == "JAM", "country"] = "JAMAICA"
FMcorrupt[FMcorrupt$wbcode == "LVA", "country"] = "LATVIA"
FMcorrupt[FMcorrupt$wbcode == "NOR", "country"] = "NORWAY"
FMcorrupt[FMcorrupt$wbcode == "PAN", "country"] = "PANAMA"
FMcorrupt[FMcorrupt$wbcode == "SWE", "country"] = "SWEDEN"
FMcorrupt[FMcorrupt$wbcode == "TUR", "country"] = "TURKEY"

# Create named regions variable using region
FMcorrupt$region_name = NA
FMcorrupt[FMcorrupt$region == 1 & ! is.na(FMcorrupt$region), "region_name"] = "Caribbean"
FMcorrupt[FMcorrupt$region == 2 & ! is.na(FMcorrupt$region), "region_name"] = "South America"
FMcorrupt[FMcorrupt$region == 3 & ! is.na(FMcorrupt$region), "region_name"] = "Europe"
FMcorrupt[FMcorrupt$region == 4 & ! is.na(FMcorrupt$region), "region_name"] = "Asia" # "South Asia"
FMcorrupt[FMcorrupt$region == 5 & ! is.na(FMcorrupt$region), "region_name"] = "Oceania"
FMcorrupt[FMcorrupt$region == 6 & ! is.na(FMcorrupt$region), "region_name"] = "Africa"
FMcorrupt[FMcorrupt$region == 7 & ! is.na(FMcorrupt$region), "region_name"] = "Middle East" # "Western Asia"

FMcorrupt$region_name = factor(FMcorrupt$region_name)

# Remove 66 rows that do not have relevant data to the key analyses
corrupt = subset(FMcorrupt, !is.na(violations) & !is.na(mission) & !is.na(staff) )


# split data in to pre and post, before and after enforcement changes
cor_pre = subset(corrupt, prepost == "pre")
cor_pos = subset(corrupt, prepost == "pos")

# Merge both the above to one line with pre and pos appeneded to variable names (prepos removed)
cor_oneline = merge(cor_pre, cor_pos, by = "wbcode", suffixes = c(".pre", ".pos"))

# Grab only the variables that are needed.
cor_oneline = cor_oneline[, c("wbcode", "violations.pre",  "violations.pos", "fines.pre", "fines.pos",
                              "mission.pre", "staff.pre", "spouse.pre", "gov_wage_gdp.pre", "pctmuslim.pre", "majoritymuslim.pre", "trade.pre",
                              "cars_total.pre", "cars_mission.pre", "pop1998.pre", "gdppcus1998.pre", "ecaid.pre", "milaid.pre", "corruption.pre", "totaid.pre",
                              "r_africa.pre", "r_middleeast.pre", "r_europe.pre", "r_southamerica.pre", "r_asia.pre",
                              "country.pre", "distUNplz.pre",
                              "region.pre", "region_name.pre"
                              )]

# Remove suffix where not needed. 
colnames(cor_oneline) =  c("wbcode", "violations.pre",  "violations.pos", "fines.pre", "fines.pos",
                              "mission", "staff", "spouse", "gov_wage_gdp", "pctmuslim", "majoritymuslim", "trade",
                              "cars_total", "cars_mission", "pop1998", "gdppcus1998", "ecaid", "milaid", "corruption", "totaid",
                              "r_africa", "r_middleeast", "r_europe", "r_southamerica", "r_asia",
                              "country", "distUNplz",
                              "region", "region_name"
                              )

# Rename FMcorrupt to ensure we don't use it accidentally
cor_nas = FMcorrupt
remove(FMcorrupt)
```

<!-- INTRODUCTION BEGIN -->

# Introduction

## Research Question
Prior to 2002 diplomats at UN missions were exampt from parking violations and fines in New York City, by virtue of their diplomatic immunity. There was wide variation in diplomats' willingness to adhere to local parking laws. This analysis attempts to understand whether the variation in adherance to local parking law was related to cultural norms in the diplomats' home countries. For the period prior to 2002, we examine the relationship between perceptions of corruption in that country and  that countries' diplomats' willingness to incur parking violations. In 2002 NYC parking enforcement acquired the right to confiscate license plates from vehicles belonging to foreign diplomats if they had accumulated unpaid parking violations, thus making payment an function of both cultral norms and legal enforcment. This had a notable compressing effect on parking law adherence. 

**Question:** Does an index of perceived corruption in the diplomats' home country have explanatory power for a given diplomatic mission's compliance with local parking regulations?

## Description of Dataset

Our data set has a total of 364 observations. Of the 364, 66 observations contain only economic data leaving NA for our dependent variable, violations. Considering the countries that are among these 66 and the variables for which they have valid data, we suspect these rows result from a merge of economic data with the violations data ($econmic\cup violations$). As such, we believe these 66 countries represent a data artefact from that data merge. As these observations do not contain valid values for key variables, we remove them from our dataset. Of note, those 66 observations appear to contain many which do not even have a mission or staff in New York City, and as such are not relevant for this study on diplomatic parking violations in New York City. Given these considerations, we feel comfortable that we are not biasing the results of the study by removing these observations. With those 66 rows removed, we're left with 298 observations (two observations for each of 149 countries where corruption data exists.) Each country has one observation from prior to the 2002 regulation change and one observation from after. Only the 'violations' and 'fines' variables differ between the two observations for a given country, while other variables remain constant. 



<!-- END INTRODUCTION -->
<!-- BEGIN UNIVARIATE ANALYSIS OF KEY VARIABLES --> 
<!--
Univariate Analysis of Key Variables (20 pts)
Use visualizations and descriptive statistics to perform a univariate analysis of each key variable. Be sure to
describe any anomalies, coding issues, or potentially erroneous values. Explain how you respond to each issue
you identify. Note any features that appear relevant to statistical analysis. Discuss what transformations
may be appropriate for each variable.
-->
# Univariate Analysis of Key Variables

Key Variables:

* violations
* corruption
* staff
* trade (maybe?)

There are 66 observations that contain only economic data leaving NA for our dependent variable, violations. Considering the countries that are among these 66 we suspect these rows result from a merge of economic data with the violations data ($econmic\cup violations$) and the economic data set had countries that did not have embasies in Manhattan.  Removing these superfluous observations results in the intersection of the two data sets ($economic\cap violations$), which is what we desire. With those 66 rows removed, we're left with 298 observations (two observations for each of 149 countries where corruption data exists.)


* show plots and summary data for key variables
    + Summary table
    + Frequency Distribution
    

```{r results="asis"}
# Use CSV version of Google Sheet 'Variable Description for Introduction': https://docs.google.com/spreadsheets/d/1cas_xxfaAY5CNGDAvBqa4ky5b8m61i3qTSgN4LrHT_g/edit#gid=621892365
variable_description = read.csv("Lab 1 - Variable Descriptions for Introduction.csv", header = TRUE, sep = ",", quote = "\"", allowEscapes = TRUE)
#summary(variable_description)

kable(variable_description, "latex", longtable = TRUE, booktabs = TRUE, caption = "Data Set Variables") %>%
  kable_styling(full_width = TRUE, latex_options = c("HOLD_position", "striped", "repeat_header"), row_label_position = 1)   

```

<!--

#Alex notes to be incorporated above

"""Data quality issues:  
~~1) We have several extra observations that have NaN values for key variables. These data likely reflect a merging 
process wherein the desired dataset regarding parking violations and fines was merged with economic data. 
+wbcode is impacted by this. There are 213 unique values for wbcode, whereas there are only 151 countries with pre and post-legislation 
observations
++++Suggestion: We drop the N/A variables and note the number of countries for which we have no data.~~

2) The violations and fines data are odd. The instructions do not provide adequate background regarding these variables.
This is a particular problem because these variable are at the core of the proposed analysis.
Oddities:
+violations has 7 decimal places. This is odd because we would expect a positive integer (in the case of a single year)
or at least a figure that is recognizable as an average of some sort (in the case of multiple years)  

3) The mission, staff, and spouse variables also contain oddities. For example, the entries for HKG and PRI are 0. These are the only zeros in 
those variables. Looking at the values for other variables for those two observations, some other oddities emerge. For example:
+HKG's majoritymuslim (which should be a dummy variable of either 1 or 0) is -1
++++Suggestion: Given that our research question regards the impact of the corruption on willingness to incur parking
violations and fines in NYC, we probably do not care about observations without a mission in NYC.
As such, taking all of these oddities and the low desirability of the data, I recommend excluding. where 
mission equals 0 or NaN  

4) The variable gov_wage_gdp is not specified in the instructions, and no source for the data is provided. We don't know if this comes from 
an official sector body or from some potentially less rigorous institution. Furthermore, the data appears somewhat disconnected from our 
core violations and fines variables, as there are numerous NaNs for gov_wage_gdp where valid values exist for violations and fines. I do think
the concept that this variable hints at - the impact of government workers' wages on their willingness to occur fines - would be interesting to
examine in relation to the degree of decline from the pre to the post subsets. The thinking being that those government workers with lower average
salaries would be less willing to incur fines out of their own pocket. This is potentially problematic though as it is not clear that the
workers themselves would definitely be the ones paying for the fines. 
++++Suggestion: Given that we don't know the provenance of this dataset nor how it was calculated, I would not treat results using this
dataset with a high degree of confidence. Rather, I would be inclined to largely exclude this variable from the analysis
At most, it should be used with caution in it's own separate section.  

5) The variable pctmuslim is among the more complete variables outside the core violations and fines dataset. We are not told the origin of this variable
either, so we have no idea of its veracity other than a common sense check. The thought behind including this variable may be that religion
and perhaps, Islam in particular, would have an impact on ethical or ethical (corrupt) behavior. It is not clear to me why Islam would be 
included and all other religions excluded. A more appropriate variable would be the percent of population which practices religion. It is 
also not clear to me that the ethics-religion association is necessarily as strong as some might think it is, though a more valid non-biased
variable could be used to test that relationship.  
"""
-->
```{r}
remove(variable_description)
```




```{r results="asis"}

# This is assuming staff is better than cars, staff+spouse, total_cars.  Though I don't know if that's safe yet.

### Probably makes sense tomove this above
round_df <- function(x, digits) {
    # round all numeric variables
    # x: data frame 
    # digits: number of digits to round
    numeric_columns <- sapply(x, mode) == 'numeric'
    x[numeric_columns] <-  round(x[numeric_columns], digits)
    x
}

summary_table_output = cor_oneline[, c("country", "staff", "violations.pre", "violations.pos", "corruption")]
summary_table_output$mean_violations_per_staff.pre = summary_table_output$violations.pre/summary_table_output$staff
summary_table_output$mean_violations_per_staff.pos = summary_table_output$violations.pos/summary_table_output$staff

tmp_rounded = round_df(summary_table_output[, c("country", "mean_violations_per_staff.pre", "mean_violations_per_staff.pos", "corruption")], 2)
tmp_rounded = tmp_rounded[order(tmp_rounded$mean_violations_per_staff.pre, decreasing = TRUE), ]
#TODO Sort, Round

kable(tmp_rounded[1:20, ], 
      "latex", longtable = TRUE, booktabs = TRUE, 
      caption = "Top 20 Countries by Parking Violations (Key Variables)", 
      col.names = c("Country", "Mean Violations per Staff Before 2002 Change", "Mean Violations per Staff After 2002 Change", "Corruption Index")) %>%
  kable_styling(full_width = TRUE, latex_options = c("HOLD_position", "striped", "repeat_header"), row_label_position = 1)   



```



```{r}
remove(summary_table_output, tmp_rounded)
```
<!-- END UNIVARIATE ANALYSIS OF KEY VARIABLES --> 

<!-- START ANALYSIS OF KEY RELATIONSHIPS --> 
<!--
Analysis of Key Relationships (30 pts)
Explore how your outcome variable is related to the other variables in your dataset. Make sure to use
visualizations to understand the nature of each bivariate relationship.
What tranformations can you apply to clarify the relationships you see in the data? Be sure to justify each
transformation you use.
-->
# Analysis of Key Relationships

Here we turn to analysis of key relationships among the variables. For the exploratory phase, we are interested in determining which of the variables are correlated, and if so, how strongly. For this analysis, where applicable, and based on the discussion in the Univariate section above, we use the log transformation of variables, as opposed to the untransformed version of the variable(s). This helps improve our understanding of relationships as positive skew that is present in some of the variables is compressed. 

1) What is the relationship between violations before and after the 2002 introduction of the new parking regulation?

```{r}
p <- ggplot(corrupt, aes(factor(region_name), violations, fill = factor(prepost))) + 
  geom_bar(stat="identity", position = "dodge") +
  scale_fill_brewer(palette = "Set1")
p + labs(title = 'The Number of Violations per Region', subtitle = 'Before and After 2002 Parking Regulation', x = 'Region', y = '# of Violations', fill='Before or After 2002 Regulation')
```
This shows us that: There is a dramatic difference between the number of violations before and after the 2002 regulation for all regions. (Regions are used here as an expedient bucketing method across the observations to reduce complexity. This is useful for explanation, though not necessarily useful for model-building.)  

However, it is difficult to see the rate-of-change relationship between the two variables given their different scales. This relationship can be explored by taking a log transformation of both variables . 

```{r}
#add log transforms
#(with the addition of 1 to the log transform to circumvent issues with the 0 values)
correlation_matrix_input$violations.pos.log = log(correlation_matrix_input$violations.pos+1) 
correlation_matrix_input$violations.pre.log = log(correlation_matrix_input$violations.pre+1)

#plot the relationship
ggplotRegression <- function (fit, title, x, y) {
  
  require(ggplot2)
  
  ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) + 
    geom_point() +
    stat_smooth(method = "lm", col = "red") +
    labs(title = title, subtitle = paste("Adj R2 = ",signif(summary(fit)$adj.r.squared, 5),
                       "Intercept =",signif(fit$coef[[1]],5 ),
                       " Slope =",signif(fit$coef[[2]], 5),
                       " P =",signif(summary(fit)$coef[2,4], 5)),
                        x = x,
                        y = y)
}

lm1 <- lm(violations.pre.log ~ violations.pos.log, data=correlation_matrix_input)
ggplotRegression(lm1, 'Comparing a Country\'s Violations Pre and Post-Regulation', 'Post-Regulation Log of Violations', 'Pre-Regulation Log of Violations')

```
This plot roughly shows that for a 1 percent increase in the number of violations before the regulation, the data reveals a roughly 0.24 percent increase in the number of violations seen after the regulation. At this exploratory stage, there are two important facets to takeaway: 1) The correlation between these two series at 0.33 is notable, 2) The relationship between the two variables is clearly positive. 

The interpretation of this is that in general countries with higher amounts of violations before the regulation are likely to also have relatively higher amounts of violations after. 

However, there is likely much more going on in this data generation process. At this early stage, we speculate that violations is likely related to the size of a mission. This facet will be examined through the total_cars and the total_people variables. We will also address some of the other variables seen in the dataset. 


<!-- END ANALYSIS OF KEY RELATIONSHIPS --> 

<!-- START ANALYSIS OF SECONDARY EFFECTS --> 
<!--
Analysis of Secondary Effects (10 pts)
What secondary variables might have confounding effects on the relationships you have identified? Explain
how these variables affect your understanding of the data.
-->
# Analysis of Secondary Effects
<!-- END ANALYSIS OF SECONDARY EFFECTS --> 

* show that pctmuslim doesn't have much relevance
* analyze trade relationship
* analyze gov_wage_gdp (particular after, when fines are being paid)
    + Countries with higher violations + higher corruption index might be low gov_wage_gdp (might also mean small governemnt, so population might also be a factor)
* does region have any *meaningful* relationship to violations?

<!-- BEGIN CONCLUSION -->
<!--
Conclusion (20 pts)
Summarize your exploratory analysis. What can you conclude based on your analysis?
-->
# Conclusion
Some poople used to be jerks.  But now they're not. Enforcing fines works on Jerks.  Canadian's are not jerks (bias: author: Hanna, is a Canadian)
<!-- END CONCLUSION -->
